{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clud ML Engine に機械学習タスクをローカルから投げるデモ\n",
    "\n",
    "題材として、Kaggle の titanic データ（ https://www.kaggle.com/c/titanic/data ）の生存者分類機をランダムフォレストで作成して予測してみます。\n",
    "\n",
    "手順としては、\n",
    "\n",
    "1. データの前処理をして Cloud Storage にデータを保存\n",
    "2. 学習用コードを書いて ML Engine にジョブを投げる\n",
    "3. Cloud Storage 上に作成されたモデルをダウンロードしてローカルでモデルをロードして予測\n",
    "\n",
    "という流れで進めています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cloud Storage にバケットを作成する\n",
    "\n",
    "データセットを格納するバケットの作成。  \n",
    "バケット名は何でもよいが、同じバケット名は使用できないので注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 何故か環境変数のPATHが通ってないので google api alient の json ファイルのパスを通しておく。  \n",
    "# この辺は個々人の環境設定によるかもしれません。\n",
    "\n",
    "import os \n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'YOUR/PATH/SERVICE_ACCOUNT_KEY.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket sklearn-sample created.\n"
     ]
    }
   ],
   "source": [
    "# 後述で subprocess を使って実行している箇所もあります。\n",
    "# せっかくPythonライブラリもあるのでこっちも使ってみたかった。\n",
    "from google.cloud import storage as gcs\n",
    "from googleapiclient import errors\n",
    "\n",
    "project_name = 'YOUR_PROJECT_NAME'\n",
    "gcs_client = gcs.Client(project_name)\n",
    "\n",
    "bucket_name = 'sklearn-sample'\n",
    "\n",
    "# バケット作成\n",
    "# すでに存在するバケット名の場合は作成エラーになるので回避\n",
    "if bucket_name not in [b.name for b in gcs_client.list_buckets()]:\n",
    "    bucket = gcs_client.create_bucket(bucket_name)\n",
    "    print('Bucket {} created.'.format(bucket.name))\n",
    "else:\n",
    "    print(\"You already own bucket {}.\".format(bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 データセットをダウンロードしてCloud Storageに格納する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle サイトからデータセットをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.6)\n",
      "train.csv: Downloaded 60KB of 60KB\r\n",
      "test.csv: Downloaded 28KB of 28KB\r\n",
      "gender_submission.csv: Downloaded 3KB of 3KB\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "DATA_DIR=\"${PWD}/dataset/\"\n",
    "kaggle competitions download -c titanic -p $DATA_DIR --force "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# アップロード用関数を作成する。サブディレクトリを指定して送信できるようひと工夫。\n",
    "def file_upload(local_filepath, bucket, bucket_filename=None):\n",
    "    \"\"\"\n",
    "    bucket: 格納先バケットオブジェクト\n",
    "    bucket_filename: バケット先に設定したいパスを含んだファイル名\n",
    "    \"\"\"\n",
    "    if bucket_filename is None:\n",
    "        # 名前がないときはtmpディレクトリを作成してローカルのファイル名と同じで格納する\n",
    "        bucket_filename = 'tmp/' + os.path.basename(local_filepath)\n",
    "    blob = bucket.blob(bucket_filename)\n",
    "    blob.upload_from_filename(local_filepath)\n",
    "    \n",
    "    \n",
    "# データ前処理を行う関数\n",
    "def preprocessing(df):\n",
    "    #欠損値処理\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "    \n",
    "    #カテゴリ変数の変換\n",
    "    df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "    \n",
    "    df = df.drop(['Cabin','Name','PassengerId','Ticket','Embarked'],axis=1)\n",
    "    df = df.astype('float64')\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データ前処理    \n",
    "train_data = './dataset/train.csv'\n",
    "test_data = './dataset/test.csv'\n",
    "\n",
    "df_train = pd.read_csv(train_data)\n",
    "df_test = pd.read_csv(test_data)\n",
    "\n",
    "df_train = preprocessing(df_train)\n",
    "df_test = preprocessing(df_test)\n",
    "\n",
    "# 前処理結果を上書きで保存\n",
    "df_train.to_csv(train_data, index=False)\n",
    "df_train.to_csv(test_data, index=False)\n",
    "\n",
    "\n",
    "# データ送信\n",
    "bucket = gcs_client.bucket(bucket_name)\n",
    "\n",
    "file_upload(train_data, bucket, bucket_filename='dataset/train.csv')\n",
    "file_upload(test_data, bucket, bucket_filename='dataset/test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. モデルとトレーニングの記述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "## 2.1 初期設定\n",
    "\n",
    "jupyter に書いた任意のセルをコードにするマジックを使う。  \n",
    "`./lib/mlcodemagic.py` にあるのでそれを使う。  \n",
    "（自作したクラスなので pip install とかできません）  \n",
    "\n",
    "\n",
    "- `%%mlcodes` でコードを記述。`run_local` と続けて記載するとローカルのjupyterでも同じコードを動かす。  \n",
    "  - 重たい学習で、ローカルで動かしたくないときは run_local を書かない\n",
    "- `%code_to_pyfile` でコードを `./trainer/task.py` に保存。これを行うまではコードは保存されない。  \n",
    "- `%clear_mlcode` でコードの初期化。`%%mlcodes` のセルを叩くたびに append されるので、同じセルをたくさん叩いちゃったときに直す用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# マジックを有効にする\n",
    "from lib.mlcodemagic import MLCodeMagic \n",
    "ip = get_ipython()\n",
    "ip.register_magics(MLCodeMagic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 モデル記述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%mlcodes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "project_name = 'YOUR_PROJECT_NAME'\n",
    "bucket_path = \"gs://sklearn-sample\"\n",
    "train_file_path = \"{}/dataset/train.csv\".format(bucket_path)\n",
    "\n",
    "# get train data \n",
    "# CloudML上からgoogle cloud storage上のファイルにアクセスする際には、tensorflowのfile_ioパッケージを使う必要がある\n",
    "with file_io.FileIO(train_file_path,'r') as f: \n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "# train valid split \n",
    "y = np.array(df['Survived'], dtype='int8')\n",
    "del df['Survived']\n",
    "X = np.array(df, dtype='float64')\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ローカル実行用データ用意\n",
    "# デモで run_local 表示するために今回は用意している。実際にはなくてもいい。\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.array(df_train['Survived'], dtype='int8')\n",
    "del df_train['Survived']\n",
    "X = np.array(df_train, dtype='float64')\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7933\n"
     ]
    }
   ],
   "source": [
    "%%mlcodes run_local\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# make model\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_validation)\n",
    "acc = accuracy_score(y_validation, y_pred)\n",
    "\n",
    "print(\"accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%mlcodes\n",
    "\n",
    "import os \n",
    "import pickle \n",
    "\n",
    "# save model \n",
    "model_name = 'model.pkl'\n",
    "\n",
    "with file_io.FileIO(os.path.join(bucket_path,'model',model_name), 'wb') as model_file:\n",
    "    pickle.dump(clf, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルコードをpyファイルに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%code_to_pyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 学習実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from datetime import datetime \n",
    "import subprocess\n",
    "\n",
    "# ジョブ定義\n",
    "# ジョブの名前は何でもいいです。 今回は job_[日付] にしました。\n",
    "bucket_path = \"gs://sklearn-sample\"\n",
    "job_name = datetime.now().strftime(\"job_%Y%m%d_%H%M%S\")\n",
    "job_dir = os.path.join(bucket_path, job_name)\n",
    "\n",
    "# runtime-version 1.4 以上でないと python 3.5 が使えないので注意\n",
    "# scale-tier BASIC_GPU で GPU指定可能\n",
    "job_cmd = \"\"\"gcloud ml-engine jobs submit training {0}\n",
    "--job-dir {1}\n",
    "--module-name trainer.task\n",
    "--package-path trainer \n",
    "--staging-bucket {2}\n",
    "--region us-central1\n",
    "--runtime-version 1.6\n",
    "--python-version 3.5\n",
    "--scale-tier BASIC_GPU\n",
    "\"\"\".format(job_name, job_dir, bucket_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ジョブ実行\n",
    "subprocess.run(job_cmd.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまで実行したらジョブが終了するまで待っていてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. 学習済みモデルのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習済みモデルをローカルにダウンロード\n",
    "model_dir = \"./models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "model_name = 'model.pkl'\n",
    "gcs_model_path = os.path.join(bucket_path, 'model', model_name)\n",
    "    \n",
    "dl_cmd = \"gsutil cp {0} {1}\".format(gcs_model_path, model_dir)\n",
    "subprocess.run(dl_cmd.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# モデルのロード\n",
    "with open(os.path.join(model_dir, model_name),'rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 予測する\n",
    "X_test = np.array(df_test, dtype='float64')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(y_pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
